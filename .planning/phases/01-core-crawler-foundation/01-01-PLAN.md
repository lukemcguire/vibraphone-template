---
phase: 01-core-crawler-foundation
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/go.mod
  - src/go.sum
  - src/urlutil/normalize.go
  - src/urlutil/normalize_test.go
  - src/urlutil/filter.go
  - src/urlutil/filter_test.go
  - src/result/result.go
autonomous: true

must_haves:
  truths:
    - "URLs with fragments are deduplicated correctly (example.com/page#section == example.com/page)"
    - "URLs with trailing slashes are deduplicated correctly (/about/ == /about)"
    - "HTTP and HTTPS variants of same URL are treated as same domain"
    - "Non-HTTP schemes (mailto:, tel:, javascript:) are filtered out silently"
    - "Subdomains are recognized as same-domain (blog.example.com is same domain as example.com)"
    - "Query parameters are preserved for deduplication (different query = different page)"
  artifacts:
    - path: "src/go.mod"
      provides: "Go module definition"
      contains: "github.com/lukemcguire/zombiecrawl"
    - path: "src/urlutil/normalize.go"
      provides: "URL normalization functions"
      exports: ["Normalize"]
    - path: "src/urlutil/normalize_test.go"
      provides: "URL normalization tests"
      contains: "TestNormalize"
    - path: "src/urlutil/filter.go"
      provides: "URL filtering and domain checking"
      exports: ["IsSameDomain", "IsHTTPScheme", "ResolveReference"]
    - path: "src/urlutil/filter_test.go"
      provides: "URL filtering tests"
      contains: "TestIsSameDomain"
    - path: "src/result/result.go"
      provides: "Shared result and link types"
      exports: ["Result", "LinkResult", "CrawlStats"]
  key_links:
    - from: "src/urlutil/normalize.go"
      to: "net/url"
      via: "URL parsing and manipulation"
      pattern: "url\\.Parse"
    - from: "src/urlutil/filter.go"
      to: "src/urlutil/normalize.go"
      via: "Normalize called before domain comparison"
      pattern: "Normalize"
---

<objective>
Create the URL utility package and shared types that the crawler depends on. URL normalization and same-domain filtering have well-defined input/output contracts making them ideal TDD candidates.

Purpose: Establish the foundational URL handling that prevents infinite crawl loops and ensures correct deduplication. These functions are called on every single URL the crawler encounters, so correctness here prevents cascading bugs.
Output: Tested urlutil package (normalize + filter), shared result types, initialized Go module.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-crawler-foundation/01-CONTEXT.md
@.planning/phases/01-core-crawler-foundation/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Go module init + result types + URL normalization (TDD)</name>
  <files>
    src/go.mod
    src/result/result.go
    src/urlutil/normalize.go
    src/urlutil/normalize_test.go
  </files>
  <action>
1. Initialize Go module in src/:
   - `cd src && go mod init github.com/lukemcguire/zombiecrawl`
   - Use latest stable Go version

2. Create src/result/result.go with shared types:
   - `LinkResult` struct: URL string, StatusCode int, Error string, SourcePage string, IsExternal bool
   - `CrawlStats` struct: TotalChecked int, BrokenCount int, Duration time.Duration
   - `Result` struct: BrokenLinks []LinkResult, Stats CrawlStats

3. TDD: URL normalization (RED then GREEN):

   **RED** - Write src/urlutil/normalize_test.go first with these test cases for a `Normalize(rawURL string) (string, error)` function:
   - Fragment stripping: "https://example.com/page#section" -> "https://example.com/page"
   - Trailing slash stripping: "https://example.com/about/" -> "https://example.com/about"
   - Root path keeps slash: "https://example.com/" -> "https://example.com/"
   - Query params preserved: "https://example.com/search?q=foo" -> "https://example.com/search?q=foo"
   - Scheme lowercased: "HTTPS://Example.Com/Page" -> "https://example.com/Page" (host lowered, path preserved)
   - Already normalized URL passes through unchanged
   - Empty string returns error
   - Invalid URL returns error

   Run tests - they MUST fail (function doesn't exist yet).
   Commit: `test(01-01): add failing tests for URL normalization`

   **GREEN** - Implement src/urlutil/normalize.go:
   - Parse with net/url.Parse
   - Strip fragment (set Fragment = "")
   - Strip trailing slash from path (unless path is "/" root)
   - Lowercase scheme and host (net/url does host already, ensure scheme)
   - Return url.String()

   Run tests - they MUST pass.
   Commit: `feat(01-01): implement URL normalization`
  </action>
  <verify>
    `cd src && go test ./urlutil/... -v` shows all normalization tests passing.
    `cd src && go vet ./...` reports no issues.
  </verify>
  <done>
    Normalize function handles all deduplication rules per user decisions: fragments stripped, trailing slashes stripped, scheme normalized, query params preserved. All tests green.
  </done>
</task>

<task type="auto">
  <name>Task 2: URL filtering and domain checking (TDD)</name>
  <files>
    src/urlutil/filter.go
    src/urlutil/filter_test.go
  </files>
  <action>
TDD: URL filtering functions (RED then GREEN):

**RED** - Write src/urlutil/filter_test.go with tests for three functions:

`IsSameDomain(targetURL string, baseHost string) bool`:
- Same host: "https://example.com/page" with base "example.com" -> true
- Subdomain match: "https://blog.example.com/post" with base "example.com" -> true
- Deep subdomain: "https://a.b.example.com/" with base "example.com" -> true
- Different domain: "https://other.com/page" with base "example.com" -> false
- Different TLD: "https://example.org/" with base "example.com" -> false
- Scheme-agnostic: "http://example.com/page" with base "example.com" -> true (per user decision: normalize scheme)
- Partial suffix mismatch: "https://notexample.com" with base "example.com" -> false (must check dot boundary)

`IsHTTPScheme(rawURL string) bool`:
- "https://example.com" -> true
- "http://example.com" -> true
- "mailto:user@example.com" -> false
- "tel:+1234567890" -> false
- "javascript:void(0)" -> false
- "ftp://files.example.com" -> false
- "" -> false

`ResolveReference(base string, ref string) (string, error)`:
- Absolute URL returned as-is: base "https://example.com", ref "https://other.com/page" -> "https://other.com/page"
- Relative path resolved: base "https://example.com/blog/", ref "post1" -> "https://example.com/blog/post1"
- Root-relative resolved: base "https://example.com/blog/", ref "/about" -> "https://example.com/about"
- Protocol-relative: base "https://example.com", ref "//cdn.example.com/file" -> "https://cdn.example.com/file"

Run tests - they MUST fail.
Commit: `test(01-01): add failing tests for URL filtering`

**GREEN** - Implement src/urlutil/filter.go:

`IsSameDomain`: Parse targetURL with net/url, extract host (strip port if present). Check if target host equals baseHost OR ends with "."+baseHost. Use strings.HasSuffix with dot boundary to prevent "notexample.com" matching "example.com".

`IsHTTPScheme`: Parse URL, check scheme is "http" or "https". Return false for empty, unparseable, or other schemes.

`ResolveReference`: Parse base URL, parse ref as reference, use baseURL.ResolveReference(refURL). Return the resolved URL string. Uses net/url.URL.ResolveReference (stdlib handles all relative resolution).

Run tests - they MUST pass.
Commit: `feat(01-01): implement URL filtering and domain checking`

Run `go mod tidy` to clean up go.mod/go.sum after all code is written.
  </action>
  <verify>
    `cd src && go test ./urlutil/... -v` shows all tests passing (both normalize and filter).
    `cd src && go test ./result/... -v` compiles successfully (no tests needed for pure types, but package must compile).
    `cd src && go vet ./...` reports no issues.
  </verify>
  <done>
    IsSameDomain correctly handles subdomains per user decision (*.example.com all match). IsHTTPScheme silently skips non-HTTP schemes per user decision. ResolveReference resolves relative URLs against base. All tests green, go vet clean.
  </done>
</task>

</tasks>

<verification>
- `cd src && go test ./... -v` passes all tests
- `cd src && go vet ./...` is clean
- go.mod declares module as github.com/lukemcguire/zombiecrawl
- URL normalization handles all user-specified deduplication rules
- Same-domain check includes subdomains per user decision
- Non-HTTP schemes filtered silently per user decision
</verification>

<success_criteria>
1. All URL normalization test cases pass (fragment, trailing slash, scheme, query param preservation)
2. All domain filtering test cases pass (subdomain matching, scheme filtering, relative resolution)
3. Result types compile and are importable by downstream packages
4. Go module initialized with correct module path
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-crawler-foundation/01-01-SUMMARY.md`
</output>
