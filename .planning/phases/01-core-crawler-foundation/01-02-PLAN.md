---
phase: 01-core-crawler-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/crawler/crawler.go
  - src/crawler/crawler_test.go
  - src/crawler/worker.go
  - src/crawler/extract.go
  - src/crawler/extract_test.go
autonomous: true

must_haves:
  truths:
    - "Crawler recursively follows same-domain anchor tags without infinite loops"
    - "External links are validated via HEAD-then-GET but not crawled further"
    - "Each URL is checked exactly once (deduplication via normalized URL)"
    - "Concurrent workers execute without goroutine leaks"
    - "Per-request timeouts prevent hanging on slow servers"
    - "4xx and 5xx responses are identified as broken links"
    - "3xx redirects are treated as valid (not broken)"
  artifacts:
    - path: "src/crawler/crawler.go"
      provides: "Core crawler orchestration (BFS loop, dedup, result collection)"
      exports: ["Crawler", "Config", "New", "Run"]
    - path: "src/crawler/worker.go"
      provides: "Worker pool with buffered channels"
      exports: ["workerPool"]
    - path: "src/crawler/extract.go"
      provides: "HTML link extraction from response bodies"
      exports: ["ExtractLinks"]
    - path: "src/crawler/extract_test.go"
      provides: "Link extraction tests"
      contains: "TestExtractLinks"
  key_links:
    - from: "src/crawler/crawler.go"
      to: "src/urlutil/normalize.go"
      via: "Normalize URLs before dedup check"
      pattern: "urlutil\\.Normalize"
    - from: "src/crawler/crawler.go"
      to: "src/urlutil/filter.go"
      via: "IsSameDomain to decide crawl vs validate"
      pattern: "urlutil\\.IsSameDomain"
    - from: "src/crawler/crawler.go"
      to: "src/result/result.go"
      via: "Collects LinkResult for broken links"
      pattern: "result\\.LinkResult"
    - from: "src/crawler/worker.go"
      to: "src/crawler/extract.go"
      via: "Workers extract links from fetched pages"
      pattern: "ExtractLinks"
    - from: "src/crawler/crawler.go"
      to: "context"
      via: "Context for cancellation and per-request timeouts"
      pattern: "context\\.WithTimeout"
---

<objective>
Build the crawler engine: worker pool with buffered channels, HTTP client with per-request timeouts, HTML link extraction, and BFS crawl coordination with deduplication.

Purpose: This is the core of zombiecrawl. The worker pool pattern must be correct from the start to avoid goroutine leaks and resource exhaustion. The crawl loop coordinates BFS traversal, deduplication, and result collection.
Output: Working crawler package that can be called with a start URL and config, returning broken link results.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-crawler-foundation/01-CONTEXT.md
@.planning/phases/01-core-crawler-foundation/01-RESEARCH.md
@.planning/phases/01-core-crawler-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: HTML link extraction + HTTP client helpers</name>
  <files>
    src/crawler/extract.go
    src/crawler/extract_test.go
    src/crawler/worker.go
  </files>
  <action>
1. Create src/crawler/extract.go - HTML link extraction:
   - `ExtractLinks(body io.Reader, baseURL *url.URL) ([]string, error)` function
   - Use golang.org/x/net/html tokenizer to find all anchor tags
   - For each `<a>` tag, extract the `href` attribute
   - Resolve relative URLs against baseURL using net/url.URL.ResolveReference
   - Filter out non-HTTP schemes using urlutil.IsHTTPScheme
   - Normalize each URL using urlutil.Normalize before returning
   - Return deduplicated list of normalized absolute URLs
   - Run `go get golang.org/x/net/html` to add dependency

2. Create src/crawler/extract_test.go with tests using strings.NewReader for HTML input:
   - Extracts absolute links: `<a href="https://example.com/page">` -> "https://example.com/page"
   - Resolves relative links: `<a href="/about">` with base "https://example.com" -> "https://example.com/about"
   - Filters non-HTTP: `<a href="mailto:user@example.com">` -> excluded from results
   - Filters javascript: `<a href="javascript:void(0)">` -> excluded
   - Handles malformed HTML gracefully (no panic)
   - Handles empty href: `<a href="">` -> base URL or excluded
   - Multiple links extracted: HTML with 3 links returns 3 URLs
   - Deduplicates within page: same link appearing twice returns once

3. Create src/crawler/worker.go - worker pool and HTTP client:
   - `Config` struct: StartURL string, Concurrency int (default 17), RequestTimeout time.Duration (default 10s)
   - Internal `crawlJob` struct: URL string, SourcePage string, IsExternal bool
   - Internal `crawlResult` struct: Job crawlJob, Links []string (discovered), Result *result.LinkResult (if broken), Err error
   - `checkURL(ctx context.Context, client *http.Client, job crawlJob) crawlResult` function:
     - For external links (job.IsExternal == true): HEAD request first. If HEAD fails or returns 405/method-not-allowed, fall back to GET. Per user decision: HEAD-first-then-GET.
     - For internal links: GET request (need body for link extraction)
     - Create per-request context with timeout: `ctx, cancel := context.WithTimeout(ctx, cfg.RequestTimeout)`
     - Always defer cancel() and resp.Body.Close()
     - Read response body for internal pages, pass to ExtractLinks
     - Classify response: 2xx/3xx = valid, 4xx/5xx = broken
     - 3xx redirects: Let http.Client follow redirects (default behavior, up to 10). Final status determines validity. Per user decision: 3xx treated as valid.
     - On error (timeout, DNS, connection): create LinkResult with error description
     - Return crawlResult with discovered links (internal) or just status (external)
  </action>
  <verify>
    `cd src && go test ./crawler/... -v` shows extraction tests passing.
    `cd src && go vet ./...` reports no issues.
  </verify>
  <done>
    ExtractLinks correctly parses HTML anchor tags, resolves relative URLs, filters non-HTTP schemes, and returns normalized deduplicated URLs. checkURL handles HEAD-then-GET for external links and full GET for internal links with per-request timeout.
  </done>
</task>

<task type="auto">
  <name>Task 2: Crawler orchestration with BFS and worker pool</name>
  <files>
    src/crawler/crawler.go
    src/crawler/crawler_test.go
  </files>
  <action>
1. Create src/crawler/crawler.go - core crawler:
   - `Crawler` struct: config Config, client *http.Client, visited sync.Map, results []result.LinkResult, mu sync.Mutex (for results slice)
   - `New(cfg Config) *Crawler` constructor:
     - Create http.Client with no global timeout (per-request timeout via context instead)
     - Configure client to NOT follow redirects for external HEAD requests (need to see 3xx status), BUT follow redirects for GET requests (default behavior)
     - Actually: use default redirect policy (follows up to 10). The final response status is what matters. 2xx/3xx final = valid.
   - `Run(ctx context.Context) (*result.Result, error)` method:
     - Create buffered channel for jobs: `jobs := make(chan crawlJob, cfg.Concurrency * 3)` (buffer ~3x worker count per research recommendation, so ~51 for 17 workers; round to sensible number)
     - Create buffered channel for results: `results := make(chan crawlResult, cfg.Concurrency * 3)`
     - Use sync.WaitGroup to track in-flight work
     - Mark start URL as visited immediately (before enqueueing) to prevent duplicates
     - Normalize start URL before first visit check

     **Worker goroutines:**
     - Launch cfg.Concurrency workers, each reading from jobs channel
     - Each worker: `for job := range jobs { result := checkURL(ctx, client, job); results <- result; wg.Done() }`
     - Workers select on ctx.Done() to respect cancellation: use `select { case job := <-jobs: ... case <-ctx.Done(): return }`

     **Coordinator goroutine:**
     - Read from results channel
     - For each crawlResult:
       - If broken (4xx, 5xx, error): append to broken links list (mutex-protected)
       - If internal page with discovered links: for each link, normalize, check visited (sync.Map LoadOrStore), if new: classify as internal/external using urlutil.IsSameDomain, wg.Add(1), enqueue to jobs channel
       - Print basic log line: "Checked: <url> [status]" for each URL (per user decision: basic logging)
     - When wg counter hits 0 (all work done): close jobs channel, close results channel

     **Shutdown coordination:**
     - Use a separate goroutine that waits on wg.Wait() then closes results channel
     - Coordinator loop exits when results channel is closed
     - On ctx cancellation (Ctrl+C): workers drain via ctx.Done(), in-flight requests complete, partial results returned

     **BFS ordering:**
     - Approximate BFS via FIFO channel is acceptable per research. Jobs channel naturally provides this.

   - `LogLine(url string, status int, err error)` helper: prints "  Checked: <url> -> <status>" or "  Checked: <url> -> ERROR: <err>" to stdout

2. Create src/crawler/crawler_test.go with integration test using httptest:
   - Set up httptest.NewServer with a handler that serves multiple pages:
     - "/" returns HTML with links to "/page1", "/page2", and "https://external.example.com"
     - "/page1" returns HTML with link to "/page2" (dedup test) and a link to a 404 page "/broken"
     - "/page2" returns HTML with no links
     - "/broken" returns 404 status
   - Create Crawler with config pointing to test server, concurrency 2 (small for test)
   - Run crawler, verify:
     - Result contains 1 broken link ("/broken" with 404 status)
     - Stats show correct TotalChecked count (should be 4 internal URLs + 1 external attempt)
     - No duplicate visits (each URL checked exactly once)
   - Test graceful cancellation: create context with cancel, cancel immediately, verify Run returns without hanging (no goroutine leak)

   Run `go get golang.org/x/sync/errgroup` if used.
  </action>
  <verify>
    `cd src && go test ./crawler/... -v -count=1` shows all tests passing (extraction + integration).
    `cd src && go test ./... -v -count=1` shows all tests across all packages passing.
    `cd src && go vet ./...` reports no issues.
    `cd src && go build ./...` compiles successfully.
  </verify>
  <done>
    Crawler recursively follows same-domain links via BFS, deduplicates via normalized URLs in sync.Map, validates external links with HEAD-then-GET, identifies 4xx/5xx as broken, treats 3xx as valid, uses worker pool with buffered channels and per-request timeouts, handles context cancellation for graceful shutdown. Integration test proves end-to-end correctness with httptest server.
  </done>
</task>

</tasks>

<verification>
- `cd src && go test ./... -v -count=1` all tests pass
- `cd src && go vet ./...` clean
- `cd src && go build ./...` compiles
- Integration test proves: recursive crawling, deduplication, external validation, broken link detection, redirect handling
- No goroutine leaks on cancellation (test with context cancel)
</verification>

<success_criteria>
1. Crawler recursively discovers and follows same-domain links without infinite loops
2. External links validated with HEAD-then-GET pattern
3. 4xx/5xx detected as broken, 3xx treated as valid
4. Worker pool uses buffered channels with configurable concurrency (default 17)
5. Per-request timeouts prevent hanging
6. Deduplication via normalized URLs (fragment, trailing slash, scheme all handled)
7. Integration test with httptest server proves correctness
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-crawler-foundation/01-02-SUMMARY.md`
</output>
