---
phase: 01-core-crawler-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - src/main.go
  - src/result/printer.go
autonomous: false

must_haves:
  truths:
    - "User can run zombiecrawl <url> and see broken links listed in terminal"
    - "Basic logging prints each URL as it is checked during crawl"
    - "Summary at end shows total checked and broken count"
    - "Broken link reports include URL, status code/error, and source page"
    - "Ctrl+C triggers graceful shutdown showing partial results"
    - "Default concurrency is 17 workers"
  artifacts:
    - path: "src/main.go"
      provides: "CLI entry point with argument parsing and signal handling"
      contains: "func main()"
    - path: "src/result/printer.go"
      provides: "Text output formatting for broken links and summary"
      exports: ["PrintResults"]
  key_links:
    - from: "src/main.go"
      to: "src/crawler/crawler.go"
      via: "Creates Crawler with Config, calls Run"
      pattern: "crawler\\.New|crawler\\.Run"
    - from: "src/main.go"
      to: "os/signal"
      via: "signal.NotifyContext for graceful shutdown"
      pattern: "signal\\.NotifyContext"
    - from: "src/main.go"
      to: "src/result/printer.go"
      via: "Prints results after crawl completes"
      pattern: "result\\.PrintResults"
    - from: "src/result/printer.go"
      to: "src/result/result.go"
      via: "Formats Result struct for display"
      pattern: "result\\.Result"
---

<objective>
Wire together the CLI entry point, signal handling, and output formatting to produce a working end-to-end zombiecrawl binary.

Purpose: This is what makes it a usable tool. The user types `zombiecrawl <url>` and gets results. Graceful shutdown ensures Ctrl+C shows partial results rather than crashing.
Output: Working CLI binary that crawls a website and reports broken links with basic text output.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-crawler-foundation/01-CONTEXT.md
@.planning/phases/01-core-crawler-foundation/01-01-SUMMARY.md
@.planning/phases/01-core-crawler-foundation/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Result printer and CLI entry point</name>
  <files>
    src/result/printer.go
    src/main.go
  </files>
  <action>
1. Create src/result/printer.go - text output formatting:
   - `PrintResults(w io.Writer, res *Result)` function:
     - If no broken links: print "No broken links found!" with a newline
     - If broken links exist: print header "Broken Links:" then for each broken link:
       - "  URL: <url>"
       - "  Status: <code>" or "  Error: <error>" (whichever applies)
       - "  Found on: <source_page>"
       - Empty line between entries
     - Always print summary line at end: "Checked <N> URLs, found <M> broken links"
     - Per user decision: broken link reports include URL, status code/error, source page
   - Use io.Writer parameter (not os.Stdout directly) for testability

2. Create src/main.go - CLI entry point:
   - Parse CLI arguments using `flag` package:
     - First positional argument (after flags): URL to crawl. If missing, print usage and exit 1.
     - `--concurrency` or `-c` flag: int, default 17 (per user decision: "17 because it's the only truly random number")
   - Validate URL: must be parseable and have http/https scheme. Print helpful error if not.
   - Set up graceful shutdown:
     - `ctx, stop := signal.NotifyContext(context.Background(), os.Interrupt, syscall.SIGTERM)`
     - `defer stop()`
   - Create crawler.Config with parsed values (StartURL, Concurrency, RequestTimeout 10s)
   - Create crawler with `crawler.New(cfg)`
   - Print start message: "Crawling <url> with <N> workers..."
   - Run crawler: `results, err := c.Run(ctx)`
   - If error and not context.Canceled: print error, exit 1
   - Print results using `result.PrintResults(os.Stdout, results)`
   - Exit code: if broken links found (len > 0), os.Exit(1). If none, os.Exit(0). Per user decision implied by DETC requirements.
   - If context was canceled (Ctrl+C): print "Crawl interrupted. Showing partial results..." before printing whatever results were collected

3. Build the binary:
   - `cd src && go build -o zombiecrawl .`
   - Verify it runs: `./zombiecrawl` (no args) should print usage
   - Verify it runs: `./zombiecrawl --help` should show flags
  </action>
  <verify>
    `cd src && go build -o zombiecrawl .` compiles successfully.
    `cd src && ./zombiecrawl` prints usage message and exits non-zero.
    `cd src && ./zombiecrawl --help` shows --concurrency flag with default 17.
    `cd src && go test ./... -v -count=1` all tests still pass.
    `cd src && go vet ./...` reports no issues.
  </verify>
  <done>
    Binary compiles and runs. Usage message shown when no URL provided. Concurrency flag defaults to 17. Signal handling set up for graceful shutdown. Results printed with broken link details and summary line.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: End-to-end verification</name>
  <files>src/main.go</files>
  <action>
Human verifies the complete zombiecrawl CLI tool end-to-end. This checkpoint confirms that recursive same-domain crawling, external link validation (HEAD-then-GET), deduplication, worker pool with configurable concurrency, per-request timeouts, graceful shutdown, and text output all work correctly against real websites.
  </action>
  <verify>
1. Build and run against a real site:
   `cd src && go build -o zombiecrawl . && ./zombiecrawl https://httpstat.us`
   Expected: crawls the site, prints each URL checked, shows summary at end.

2. Test with example.com:
   `./zombiecrawl https://example.com`
   Expected: completes quickly (few links), shows summary.

3. Test concurrency flag:
   `./zombiecrawl -c 5 https://example.com`
   Expected: starts with "Crawling https://example.com with 5 workers..."

4. Test no-args usage:
   `./zombiecrawl`
   Expected: prints usage message, exits with non-zero code.

5. Test Ctrl+C: start crawling a larger site, press Ctrl+C mid-crawl.
   Expected: prints "Crawl interrupted. Showing partial results..." then shows partial results.

6. Verify all tests pass:
   `cd src && go test ./... -v -count=1`
  </verify>
  <done>
    User confirms: tool crawls real sites, prints broken links with details, shows summary, handles Ctrl+C gracefully, concurrency flag works. Type "approved" or describe issues.
  </done>
</task>

</tasks>

<verification>
- Binary compiles and runs with `zombiecrawl <url>`
- Usage message on no args
- Concurrency flag works with default 17
- Broken links show URL, status/error, source page
- Summary line at end: "Checked N URLs, found M broken links"
- Ctrl+C shows partial results
- All tests pass across all packages
</verification>

<success_criteria>
1. `zombiecrawl <url>` crawls a real website and prints results
2. Broken links listed with URL, status code/error, and source page
3. Summary shows total checked and broken count
4. Default concurrency is 17
5. Ctrl+C triggers graceful shutdown with partial results
6. Exit code 1 when broken links found, 0 when none
7. All unit and integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-crawler-foundation/01-03-SUMMARY.md`
</output>
