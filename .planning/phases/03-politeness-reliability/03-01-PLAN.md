---
phase: 03-politeness-reliability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/result/errors.go
  - src/crawler/crawler.go
  - src/crawler/worker.go
autonomous: true
requirements:
  - DETC-03
  - DETC-04

must_haves:
  truths:
    - "Broken links are categorized by error type (timeout, DNS failure, connection refused, 4xx, 5xx, redirect loop)"
    - "Crawler respects rate limit before making any HTTP request"
  artifacts:
    - path: "src/result/errors.go"
      provides: "ErrorCategory type and ClassifyError function"
      min_lines: 40
    - path: "src/crawler/crawler.go"
      provides: "Rate limiter integration in Crawler struct"
      contains: "rate.Limiter"
  key_links:
    - from: "src/crawler/worker.go"
      to: "src/result/errors.go"
      via: "ClassifyError function call"
      pattern: "ClassifyError"
    - from: "src/crawler/crawler.go"
      to: "golang.org/x/time/rate"
      via: "limiter.Wait(ctx)"
      pattern: "limiter\\.Wait"
---

<objective>
Add error classification system and rate limiting to the crawler foundation.

Purpose: Enables detailed error categorization for reporting and polite request pacing.
Output: Error types, classification function, rate limiter in crawler.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-politeness-reliability/03-CONTEXT.md
@.planning/phases/03-politeness-reliability/03-RESEARCH.md

@src/result/result.go
@src/crawler/crawler.go
@src/crawler/worker.go
@src/crawler/events.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create error classification types and function</name>
  <files>src/result/errors.go</files>
  <action>
Create a new file `src/result/errors.go` with error categorization:

1. Define `ErrorCategory` type as string with constants:
   - `CategoryTimeout` = "timeout"
   - `CategoryDNSFailure` = "dns_failure"
   - `CategoryConnectionRefused` = "connection_refused"
   - `Category4xx` = "4xx"
   - `Category5xx` = "5xx"
   - `CategoryRedirectLoop` = "redirect_loop"
   - `CategoryUnknown` = "unknown"

2. Implement `ClassifyError(err error, statusCode int, isRedirectLoop bool) ErrorCategory`:
   - If isRedirectLoop is true, return CategoryRedirectLoop
   - If statusCode > 0: 400-499 returns Category4xx, 500+ returns Category5xx
   - Use errors.Is(err, context.DeadlineExceeded) for timeout detection
   - Use errors.As for *net.DNSError for DNS failures
   - Use errors.As for *net.OpError, check for "connection refused" string or Timeout()
   - Return CategoryUnknown as fallback

3. Implement `FormatCategory(cat ErrorCategory) string` for display:
   - timeout -> "Timeouts"
   - dns_failure -> "DNS Failures"
   - connection_refused -> "Connection Refused"
   - 4xx -> "Client Errors (4xx)"
   - 5xx -> "Server Errors (5xx)"
   - redirect_loop -> "Redirect Loops"
   - unknown -> "Other Errors"

Use standard library only (errors, net, context packages). Do NOT add external dependencies.
  </action>
  <verify>
Run tests:
```
cd src && go test ./result/... -v
```
Create a simple test that verifies ClassifyError returns correct categories for known error types.
  </verify>
  <done>
ErrorCategory type exists with all 7 category constants, ClassifyError correctly categorizes errors, FormatCategory produces readable labels.
</done>
</task>

<task type="auto">
  <name>Task 2: Add rate limiting to the crawler</name>
  <files>src/crawler/crawler.go, src/crawler/worker.go, src/go.mod</files>
  <action>
Integrate token-bucket rate limiting:

1. Add `golang.org/x/time/rate` dependency:
   ```
   cd src && go get golang.org/x/time/rate
   ```

2. Update `Config` struct in `src/crawler/worker.go`:
   - Add `RateLimit int` field (requests per second, default 10)
   - Add `UserAgent string` field (default "zombiecrawl/1.0")

3. Update `Crawler` struct in `src/crawler/crawler.go`:
   - Add `limiter *rate.Limiter` field

4. Update `New()` function:
   - Create limiter: `rate.NewLimiter(rate.Limit(cfg.RateLimit), cfg.RateLimit)`
   - Set default RateLimit to 10 if <= 0
   - Set default UserAgent to "zombiecrawl/1.0 (+https://github.com/lukemcguire/zombiecrawl)" if empty

5. Create internal `checkURLWithLimiter` method that:
   - Calls `c.limiter.Wait(ctx)` before making HTTP request
   - Returns early if context cancelled while waiting
   - Delegates to existing CheckURL logic

6. Update the worker goroutine in Run() to use the limiter:
   - Before calling CheckURL, call `c.limiter.Wait(ctx)`
   - If Wait returns error (context cancelled), skip the job

CRITICAL: The limiter must be shared across all workers (stored in Crawler struct), not created per-worker.
  </action>
  <verify>
Run tests:
```
cd src && go test ./crawler/... -v
```
Verify rate limiting works by checking that limiter.Wait is called before HTTP requests.
  </verify>
  <done>
Rate limiter integrated, all workers share single limiter, requests are paced at configured rate (default 10/sec).
</done>
</task>

</tasks>

<verification>
- `go test ./...` passes in src directory
- ErrorCategory type with all categories defined
- ClassifyError handles all error types correctly
- Rate limiter shared across workers
- Default rate limit of 10 req/sec applied
</verification>

<success_criteria>
- Error categories defined and classification working
- Rate limiter integrated with shared instance across workers
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-politeness-reliability/03-01-SUMMARY.md`
</output>
