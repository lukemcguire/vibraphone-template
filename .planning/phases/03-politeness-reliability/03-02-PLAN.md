---
phase: 03-politeness-reliability
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/crawler/robots.go
  - src/crawler/crawler.go
  - src/crawler/worker.go
  - src/go.mod
autonomous: true
requirements:
  - CRWL-07

must_haves:
  truths:
    - "Crawler respects robots.txt disallow directives"
    - "robots.txt is cached per host for 1 hour"
    - "Missing robots.txt (404) allows all crawling"
    - "Failed robots.txt fetch (timeout, 5xx) allows all crawling"
  artifacts:
    - path: "src/crawler/robots.go"
      provides: "RobotsChecker with cache and allowed checking"
      min_lines: 80
      contains: "Allowed(ctx context.Context, rawURL, userAgent string)"
    - path: "src/crawler/crawler.go"
      provides: "RobotsChecker integration in Crawler"
      contains: "robotsChecker"
  key_links:
    - from: "src/crawler/crawler.go"
      to: "src/crawler/robots.go"
      via: "c.robotsChecker.Allowed call"
      pattern: "robotsChecker\\.Allowed"
    - from: "src/crawler/robots.go"
      to: "github.com/temoto/robotstxt"
      via: "FromStatusAndBytes"
      pattern: "FromStatusAndBytes"
---

<objective>
Implement robots.txt compliance with caching for ethical crawling.

Purpose: Respects site owner crawling preferences via robots.txt directives.
Output: RobotsChecker type with 1-hour cache, integration into crawler.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-politeness-reliability/03-CONTEXT.md
@.planning/phases/03-politeness-reliability/03-RESEARCH.md

@src/crawler/crawler.go
@src/crawler/worker.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create RobotsChecker with caching</name>
  <files>src/crawler/robots.go, src/go.mod</files>
  <action>
Create `src/crawler/robots.go` with robots.txt handling:

1. Add dependency:
   ```
   cd src && go get github.com/temoto/robotstxt
   ```

2. Create `cachedRobots` struct:
   ```go
   type cachedRobots struct {
       data      *robotstxt.RobotsData
       fetchedAt time.Time
   }
   ```

3. Create `RobotsChecker` struct:
   ```go
   type RobotsChecker struct {
       client *http.Client
       cache  sync.Map // host string -> *cachedRobots
       cacheTTL time.Duration // default 1 hour
   }
   ```

4. Implement `NewRobotsChecker(client *http.Client) *RobotsChecker`:
   - Set cacheTTL to 1 hour
   - Store provided client

5. Implement `Allowed(ctx context.Context, rawURL, userAgent string) (bool, error)`:
   - Parse URL to extract host and path
   - Check cache for host entry with valid TTL (time.Since(fetchedAt) < cacheTTL)
   - If cached entry exists with nil data, return true (allow-all case)
   - If cached entry exists with data, return data.TestAgent(path, userAgent)
   - If not cached, fetch robots.txt from `{scheme}://{host}/robots.txt`
   - On fetch error (timeout, network): cache nil entry, return true (proceed freely)
   - On HTTP response: use `robotstxt.FromStatusAndBytes(statusCode, body)` which handles:
     - 2xx: parse and apply rules
     - 404: returns nil (allow all)
     - 5xx: returns nil (allow all)
   - Cache result and return robots.TestAgent(path, userAgent)

6. Implement `ClearCache()` for testing purposes

Key: Use FromStatusAndBytes which handles 404/5xx as allow-all per user decision.
  </action>
  <verify>
Run tests:
```
cd src && go test ./crawler/... -v -run Robots
```
Create test cases for:
- robots.txt disallows specific path
- robots.txt allows all paths
- robots.txt 404 (allow all)
- robots.txt timeout (allow all)
- cache expiration and refresh
  </verify>
  <done>
RobotsChecker fetches and caches robots.txt, respects directives, handles 404/5xx gracefully, 1-hour cache TTL.
</done>
</task>

<task type="auto">
  <name>Task 2: Integrate RobotsChecker into crawler</name>
  <files>src/crawler/crawler.go</files>
  <action>
Wire RobotsChecker into the crawler:

1. Add `robotsChecker *RobotsChecker` field to `Crawler` struct

2. In `New()` function:
   - Create RobotsChecker with the HTTP client: `NewRobotsChecker(&http.Client{Timeout: 5 * time.Second})`
   - Note: Use separate client with shorter timeout for robots.txt to avoid blocking

3. In `Run()` method, before enqueueing discovered internal links:
   - Call `c.robotsChecker.Allowed(ctx, normalized, c.cfg.UserAgent)`
   - If not allowed, skip the URL (do not add to jobs channel)
   - If error, proceed anyway (don't block on robots.txt failure)

4. For the start URL, also check robots.txt before beginning the crawl

Add the check in the coordinator loop where new jobs are created:
```go
if !cr.Job.IsExternal {
    for _, link := range cr.Links {
        normalized, _ := urlutil.Normalize(link)
        if _, loaded := c.visited.LoadOrStore(normalized, true); loaded {
            continue
        }
        // Check robots.txt before enqueueing
        if allowed, _ := c.robotsChecker.Allowed(ctx, normalized, c.cfg.UserAgent); !allowed {
            continue // Skip disallowed URLs
        }
        // ... enqueue job
    }
}
```
  </action>
  <verify>
Run tests:
```
cd src && go test ./crawler/... -v
```
Manual test:
```
go run . https://httpbin.org
```
Verify robots.txt is checked (check logs or add debug output).
  </verify>
  <done>
Crawler checks robots.txt before crawling each URL, skips disallowed paths, handles failures gracefully.
</done>
</task>

</tasks>

<verification>
- `go test ./...` passes
- RobotsChecker correctly parses and caches robots.txt
- Disallowed URLs are skipped
- 404/5xx robots.txt responses allow all
- Cache expires after 1 hour
</verification>

<success_criteria>
- robots.txt compliance working
- Cache implemented with 1-hour TTL
- Graceful handling of missing/unreachable robots.txt
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-politeness-reliability/03-02-SUMMARY.md`
</output>
