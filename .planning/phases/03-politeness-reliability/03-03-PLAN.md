---
phase: 03-politeness-reliability
plan: 03
type: execute
wave: 2
depends_on:
  - "03-01"
  - "03-02"
files_modified:
  - src/crawler/worker.go
  - src/crawler/retry.go
  - src/result/result.go
  - src/crawler/events.go
autonomous: true
requirements:
  - DETC-06
  - DETC-07
  - DETC-08

must_haves:
  truths:
    - "Transient errors are retried with exponential backoff (1s, 2s, 4s...)"
    - "4xx errors (except 429) are NOT retried"
    - "Redirect loops are detected and reported as broken links"
    - "Each broken link shows its error category and source page"
  artifacts:
    - path: "src/crawler/retry.go"
      provides: "RetryPolicy and CheckURLWithRetry function"
      min_lines: 60
      contains: "CheckURLWithRetry"
    - path: "src/crawler/worker.go"
      provides: "CheckRedirect function for loop detection"
      contains: "CheckRedirect"
    - path: "src/result/result.go"
      provides: "ErrorCategory field in LinkResult"
      contains: "ErrorCategory"
  key_links:
    - from: "src/crawler/worker.go"
      to: "src/crawler/retry.go"
      via: "CheckURLWithRetry call"
      pattern: "CheckURLWithRetry"
    - from: "src/crawler/worker.go"
      to: "src/result/errors.go"
      via: "ClassifyError call"
      pattern: "ClassifyError"
---

<objective>
Add retry logic with exponential backoff and redirect loop detection.

Purpose: Handles transient failures gracefully while detecting true broken links.
Output: Retry wrapper, redirect loop detection, error categories in results.
</objective>

<execution_context>
@/home/luke/.claude/get-shit-done/workflows/execute-plan.md
@/home/luke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-politeness-reliability/03-CONTEXT.md
@.planning/phases/03-politeness-reliability/03-RESEARCH.md

@src/crawler/crawler.go
@src/crawler/worker.go
@src/result/result.go
@src/result/errors.go
@src/crawler/events.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add ErrorCategory to LinkResult and CrawlEvent</name>
  <files>src/result/result.go, src/crawler/events.go</files>
  <action>
Extend result types with error categories:

1. Update `LinkResult` in `src/result/result.go`:
   - Add `ErrorCategory ErrorCategory` field
   - Import the result package's ErrorCategory type

2. Update `CrawlEvent` in `src/crawler/events.go`:
   - Add `ErrorCategory result.ErrorCategory` field
   - Import "github.com/lukemcguire/zombiecrawl/result"

This enables error categories to flow from CheckURL through to the final output.
  </action>
  <verify>
Run tests:
```
cd src && go build ./...
```
Verify compilation succeeds with new fields.
  </verify>
  <done>
LinkResult and CrawlEvent have ErrorCategory fields.
</done>
</task>

<task type="auto">
  <name>Task 2: Implement redirect loop detection in CheckURL</name>
  <files>src/crawler/worker.go</files>
  <action>
Add redirect loop detection to CheckURL:

1. Create a per-request HTTP client with CheckRedirect:
   ```go
   var isRedirectLoop bool
   var visitedInChain []string

   loopClient := &http.Client{
       Timeout: cfg.RequestTimeout,
       CheckRedirect: func(req *http.Request, via []*http.Request) error {
           currentURL := req.URL.String()

           // Check if we've seen this URL in the current chain
           for _, v := range visitedInChain {
               if v == currentURL {
                   isRedirectLoop = true
                   return http.ErrUseLastResponse
               }
           }
           visitedInChain = append(visitedInChain, currentURL)

           // Also limit total redirects (10 is Go default)
           if len(via) >= 10 {
               isRedirectLoop = true
               return errors.New("stopped after 10 redirects")
           }
           return nil
       },
   }
   ```

2. Use `loopClient` instead of the passed `client` in CheckURL

3. After request completes, if isRedirectLoop is true:
   - Set Result.ErrorCategory to CategoryRedirectLoop
   - Set Result.Error to "redirect loop detected"
   - Return the result

4. Import "errors" package for the error message

The CheckRedirect function is called before each redirect follows, allowing us to detect cycles.
  </action>
  <verify>
Run tests:
```
cd src && go test ./crawler/... -v
```
Test redirect loop detection with a URL that causes loops (may need mock server).
  </verify>
  <done>
Redirect loops are detected and reported as broken links with CategoryRedirectLoop.
</done>
</task>

<task type="auto">
  <name>Task 3: Implement retry logic with exponential backoff</name>
  <files>src/crawler/retry.go, src/crawler/worker.go</files>
  <action>
Create `src/crawler/retry.go` with retry logic:

1. Create `RetryPolicy` struct:
   ```go
   type RetryPolicy struct {
       MaxRetries int           // 2 = 3 total attempts
       BaseDelay  time.Duration // 1s
       MaxDelay   time.Duration // 30s cap
   }
   ```

2. Create `DefaultRetryPolicy() RetryPolicy`:
   - MaxRetries: 2
   - BaseDelay: 1 * time.Second
   - MaxDelay: 30 * time.Second

3. Create `CheckURLWithRetry(ctx context.Context, client *http.Client, job CrawlJob, cfg Config, policy RetryPolicy) CrawlResult`:
   - Initialize backoff = policy.BaseDelay
   - Loop from attempt 0 to policy.MaxRetries (inclusive):
     - If attempt > 0: wait with `select { case <-ctx.Done(): return; case <-time.After(backoff): }`
     - Double backoff each retry: `backoff = min(backoff * 2, policy.MaxDelay)`
     - Call CheckURL and store result
     - If no error and status < 400: return success
     - Call `shouldRetry(result)` - if false, return immediately
     - Store lastError for final message
   - After all retries: append retry info to error message, return last result

4. Create `shouldRetry(res CrawlResult) bool`:
   - If StatusCode == 0 (network error): return isRetryableNetworkError(res.Err or res.Result.Error)
   - If StatusCode == 429: return true (rate limited)
   - If StatusCode >= 500: return true (server error)
   - Otherwise: return false (4xx except 429 are NOT retried)

5. Create `isRetryableNetworkError(err error) bool`:
   - If err is nil: return false
   - If errors.Is(err, context.DeadlineExceeded): return true
   - If errors.As for *net.OpError: return true (covers timeout, connection refused)
   - If errors.As for *net.DNSError: return true
   - Otherwise: return false

6. Update Config to include RetryPolicy field with default

7. Update crawler to use CheckURLWithRetry instead of CheckURL directly

8. Classify errors before returning:
   - Call `result.ClassifyError(err, statusCode, isRedirectLoop)` and set ErrorCategory
  </action>
  <verify>
Run tests:
```
cd src && go test ./crawler/... -v -run Retry
```
Verify retry behavior:
- Network errors retry 3 times
- 5xx errors retry 3 times
- 429 errors retry 3 times
- 404 errors do NOT retry
  </verify>
  <done>
Retry logic with exponential backoff (1s, 2s, 4s...) working, transient errors retry 3 times, 4xx (except 429) skip retry.
</done>
</task>

</tasks>

<verification>
- `go test ./...` passes
- Redirect loops detected and categorized
- Network errors retry with exponential backoff
- 5xx and 429 errors retry
- 4xx errors (except 429) do not retry
- Error categories set on all broken link results
</verification>

<success_criteria>
- Retry logic working with 2 retries (3 attempts)
- Exponential backoff: 1s, 2s, 4s...
- Redirect loop detection functional
- Error categories populated in results
</success_criteria>

<output>
After completion, create `.planning/phases/03-politeness-reliability/03-03-SUMMARY.md`
</output>
